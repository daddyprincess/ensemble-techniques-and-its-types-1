{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0addc9c5-7045-4f93-b555-69b8f0627745",
   "metadata": {},
   "source": [
    "## Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d65a8-ae87-4111-9202-7bfbf5682975",
   "metadata": {},
   "outputs": [],
   "source": [
    "An ensemble technique in machine learning is a method that combines the predictions or decisions of multiple individual\n",
    "models (often called \"base models\" or \"weak learners\") to produce a single, more robust and accurate prediction or decision.\n",
    "The idea behind ensemble methods is that by combining the diverse viewpoints of multiple models, you can often achieve\n",
    "better overall performance than with any single model alone. Ensemble techniques are widely used in machine learning because\n",
    "they can significantly improve predictive accuracy and generalization.\n",
    "\n",
    "Key characteristics of ensemble techniques include:\n",
    "\n",
    "1.Diversity: Ensemble models should be diverse in some way. This diversity can come from using different algorithms, \n",
    "  different subsets of data, or by introducing randomness during training.\n",
    "\n",
    "2.Combining Mechanism: Ensembles combine individual model predictions through a specific mechanism, such as averaging,\n",
    " voting, or weighting. The choice of combination method depends on the type of problem (classification, regression, etc.) \n",
    "and the ensemble method used.\n",
    "\n",
    "3.Stability and Robustness: Ensemble methods often result in more stable and robust models that are less prone to \n",
    "  overfitting, especially when compared to individual models with high variance.\n",
    "\n",
    "Common ensemble techniques in machine learning include:\n",
    "\n",
    "1.Bagging (Bootstrap Aggregating):\n",
    "\n",
    "    ~Bagging involves training multiple instances of the same model on different subsets of the training data (with\n",
    "     replacement) and averaging or voting on their predictions.\n",
    "    ~Random Forest is a popular example of a bagging ensemble technique, using decision trees as base models.\n",
    "    \n",
    "2.Boosting:\n",
    "\n",
    "    ~Boosting focuses on training multiple models sequentially, where each model gives more weight to examples that the\n",
    "     previous model misclassified. This helps to correct errors over iterations.\n",
    "    ~Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "    \n",
    "3.Stacking (Stacked Generalization):\n",
    "\n",
    "    ~Stacking combines multiple models by training a meta-model on their predictions. The base models make predictions on \n",
    "     the input data, and the meta-model is trained to make a final prediction based on these base model outputs.\n",
    "    ~Stacking can be used with a variety of base models and meta-models.\n",
    "    \n",
    "4.Voting Classifiers/Regression:\n",
    "\n",
    "    ~Voting classifiers (for classification) and voting regressors (for regression) combine predictions from multiple\n",
    "     models by either majority voting (for classification) or averaging (for regression).\n",
    "    ~Different voting strategies like hard voting (majority) and soft voting (weighted) are used.\n",
    "    \n",
    "5.Ensemble of Neural Networks:\n",
    "\n",
    "    ~In deep learning, ensembles of neural networks can be created by training multiple neural networks with different\n",
    "     architectures or initializations and combining their predictions.\n",
    "        \n",
    "Ensemble techniques are powerful tools for improving model performance and are commonly used in various machine learning \n",
    "competitions and real-world applications. They are especially effective when applied to a set of base models that have \n",
    "complementary strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed19f07-9d12-4033-8732-64438c64db7f",
   "metadata": {},
   "source": [
    "## Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e10dd-bcce-47f8-af41-53073df90361",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble techniques are used in machine learning for several important reasons:\n",
    "\n",
    "1.Improved Predictive Performance:\n",
    "\n",
    "    ~One of the primary motivations for using ensemble techniques is to improve predictive performance. Ensembles often\n",
    "     achieve higher accuracy and generalization than individual models, leading to better overall results.\n",
    "        \n",
    "2.Reduction of Overfitting:\n",
    "\n",
    "    ~Ensembles are effective at reducing overfitting, especially when individual base models have high variance or when\n",
    "     trained on limited data. By combining multiple models, ensembles can provide more stable and reliable predictions.\n",
    "        \n",
    "3.Robustness:\n",
    "\n",
    "    ~Ensembles are robust against outliers and noisy data. Outliers that may have a strong influence on a single model's\n",
    "     prediction are less likely to significantly impact the ensemble's prediction because they are averaged out or have \n",
    "    less weight.\n",
    "    \n",
    "4.Handling Model Bias:\n",
    "\n",
    "    ~Ensemble techniques can help overcome bias in individual models. By combining models with different sources of error,\n",
    "     ensemble methods can mitigate the impact of any single model's biases.\n",
    "        \n",
    "5.Enhanced Generalization:\n",
    "\n",
    "    ~Ensembles generalize well to new, unseen data. They capture a broader range of patterns and relationships in the data,\n",
    "    making them more likely to make accurate predictions on data they haven't seen before.\n",
    "    \n",
    "6.Versatility:\n",
    "\n",
    "    ~Ensemble methods can be applied to a wide range of machine learning algorithms and models, including decision trees,\n",
    "     neural networks, support vector machines, and more. This versatility allows practitioners to leverage the strengths of\n",
    "    different algorithms.\n",
    "    \n",
    "7.Interpretability:\n",
    "\n",
    "    ~Ensembles can sometimes provide better interpretability than individual complex models. By combining simpler base\n",
    "     models, it may be easier to understand how different features contribute to predictions.\n",
    "        \n",
    "8.Reduction of Model Variance:\n",
    "\n",
    "    ~In bagging ensembles, like Random Forest, the variance of predictions is often reduced because base models are trained\n",
    "     on different subsets of data, introducing diversity and mitigating the impact of overfitting.\n",
    "        \n",
    "9.Model Selection and Tuning:\n",
    "\n",
    "    ~Ensembles can help with model selection and tuning. By trying different models as base learners and combining them,\n",
    "     practitioners can explore a wider range of model possibilities.\n",
    "        \n",
    "10.Winning Machine Learning Competitions:\n",
    "\n",
    "    ~Ensembles are a common and effective strategy in machine learning competitions (Kaggle, etc.) because they frequently\n",
    "     lead to improved performance and can provide an edge in highly competitive settings.\n",
    "        \n",
    "11.Reduction of Bias:\n",
    "\n",
    "    ~In boosting ensembles, models are trained to correct the errors of previous models. This can help reduce bias and\n",
    "     improve the accuracy of predictions.\n",
    "        \n",
    "Overall, ensemble techniques are a valuable tool in machine learning because they harness the wisdom of multiple models to\n",
    "enhance predictive accuracy, robustness, and generalization. They are particularly useful when the base models have \n",
    "complementary strengths and weaknesses, allowing the ensemble to compensate for individual model limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a99015-42a5-4990-bc88-79654969e20e",
   "metadata": {},
   "source": [
    "## Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9576d18-b4c8-44f0-8421-8e66b04e2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble machine learning technique used to improve the accuracy and\n",
    "robustness of predictive models. It works by training multiple instances of the same base model on different subsets of\n",
    "the training data and then combining their predictions to make more accurate and stable predictions.\n",
    "\n",
    "The key steps in bagging are as follows:\n",
    "\n",
    "1.Bootstrap Sampling: Bagging begins by creating multiple random subsets (samples) of the training data, each of which is \n",
    "  generated through a process called bootstrap sampling. Bootstrap sampling involves randomly selecting data points from \n",
    "the original training dataset with replacement. As a result, some data points may appear multiple times in a single subset,\n",
    "while others may not be included at all.\n",
    "\n",
    "2.Base Model Training: For each bootstrap sample, a base model (e.g., a decision tree, a neural network, or any other \n",
    "  machine learning algorithm) is trained independently on that sample. This means that multiple base models are trained,\n",
    "each on a different subset of the data.\n",
    "\n",
    "3.Predictions: Once the base models are trained, they are used to make predictions on the validation or test data (or out-\n",
    "  of-bag samples, which are data points not included in the bootstrap sample used for training).\n",
    "\n",
    "4.Combination of Predictions: The predictions from each base model are then combined in some way to produce a final ensemble\n",
    "  prediction. The combination method depends on the type of problem:\n",
    "\n",
    "        ~For classification problems, a common approach is to use majority voting, where the class that receives the most \n",
    "         votes among the base models is selected as the final prediction.\n",
    "        ~For regression problems, the ensemble prediction can be calculated by averaging the predictions from the base \n",
    "        models.\n",
    "        \n",
    "The benefits of bagging include:\n",
    "\n",
    "    ~Reduced Variance: Bagging reduces the variance of the model by training on different subsets of data, which means the \n",
    "     model is less likely to overfit the training data.\n",
    "\n",
    "    ~Improved Robustness: Bagging is more robust to outliers and noisy data because it reduces their impact by averaging\n",
    "     over multiple models.\n",
    "\n",
    "    ~Increased Accuracy: By combining the predictions of multiple models, bagging often leads to improved predictive \n",
    "     accuracy compared to using a single model.\n",
    "\n",
    "    ~Parallelization: The base models can be trained independently, making bagging amenable to parallelization, which can\n",
    "    lead to faster training times.\n",
    "\n",
    "A well-known example of bagging is the Random Forest algorithm, which uses an ensemble of decision trees trained using\n",
    "bagging. Random Forests are widely used in various machine learning applications due to their robustness and excellent \n",
    "predictive performance. Bagging can also be applied to other base models, making it a versatile technique in ensemble\n",
    "learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8da5e6-1935-4ab6-b14e-e1b66cc55f83",
   "metadata": {},
   "source": [
    "## Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13daf79-1435-44c0-ac2b-d151c1360d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble machine learning technique that aims to improve the accuracy of predictive models by combining the\n",
    "predictions of multiple base models (often called \"weak learners\" or \"base classifiers\") in a sequential and adaptive\n",
    "manner. Unlike bagging, where base models are trained independently, boosting trains base models sequentially, with each\n",
    "new model focusing on the examples that previous models struggled with. The key idea behind boosting is to give more weight\n",
    "to the training instances that are misclassified or have higher errors, thereby focusing the subsequent models on those \n",
    "instances.\n",
    "\n",
    "Here's how boosting typically works:\n",
    "\n",
    "1.Base Model Training (Weak Learners): Boosting starts by training a base model on the original training data. This base\n",
    "model is often a simple and weak learner, such as a decision stump (a decision tree with just one split) or a shallow neural\n",
    "network.\n",
    "\n",
    "2.Weighted Data: After the initial base model is trained, the training data is assigned weights. Initially, all data points\n",
    "have equal weights.\n",
    "\n",
    "3.Sequential Model Building: Boosting builds a sequence of base models sequentially, where each new model is trained on a\n",
    "modified version of the training data. The modification involves adjusting the weights of the training instances. The\n",
    "weights of misclassified instances are increased, while the weights of correctly classified instances are decreased.\n",
    "\n",
    "4.Combining Predictions: After all base models are trained, their predictions are combined to make a final ensemble \n",
    "prediction. The combination can be done through weighted voting or weighted averaging, where models with better performance\n",
    "typically have higher weights.\n",
    "\n",
    "5.Final Prediction: The final prediction of the ensemble is typically the result of aggregating the individual predictions\n",
    "of the base models, often weighted by their performance on the training data.\n",
    "\n",
    "The boosting process continues iteratively until a stopping criterion is met. This stopping criterion can be a predefined\n",
    "number of iterations (base models) or when the performance on the training data reaches a satisfactory level.\n",
    "\n",
    "Common boosting algorithms include:\n",
    "\n",
    "1.AdaBoost (Adaptive Boosting): AdaBoost is one of the earliest and most well-known boosting algorithms. It adjusts the\n",
    "weights of training instances to focus on misclassified examples and combines base models using weighted voting.\n",
    "\n",
    "2.Gradient Boosting: Gradient Boosting builds base models in a way that minimizes the error gradient. Popular variants of \n",
    "gradient boosting include Gradient Boosting Machines (GBM), XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "3.Stochastic Gradient Boosting (SGD): This is a variant of gradient boosting that uses stochastic gradient descent as the\n",
    "optimization method. It is particularly effective for large datasets.\n",
    "\n",
    "Boosting is known for its ability to improve the performance of relatively simple base models, and it often achieves state-\n",
    "of-the-art results in various machine learning tasks, including classification and regression. However, boosting algorithms\n",
    "can be sensitive to noisy data and outliers, and overfitting can occur if not carefully tuned or if the number of iterations\n",
    "is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b42d73-ce5f-4d80-8b4d-7e949673b564",
   "metadata": {},
   "source": [
    "## Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7e4a4-833c-4c32-bd70-71036361ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble techniques offer several benefits in machine learning, making them powerful tools for improving predictive \n",
    "modeling and addressing various challenges. Here are some key benefits of using ensemble techniques:\n",
    "\n",
    "1.Improved Predictive Performance:\n",
    "\n",
    "    ~Ensemble methods often lead to higher accuracy and better generalization than individual models. They combine multiple\n",
    "     base models, each capturing different aspects of the data, resulting in more robust predictions.\n",
    "        \n",
    "2.Reduction of Overfitting:\n",
    "\n",
    "    ~Ensembles are effective at reducing overfitting, especially when individual base models have high variance or when the \n",
    "     dataset is small. By combining multiple models, ensembles can provide more stable and less biased predictions.\n",
    "        \n",
    "3.Robustness to Noise and Outliers:\n",
    "\n",
    "    ~Ensembles are robust to noisy data and outliers because they reduce the impact of individual data points that may\n",
    "     have unusual characteristics or errors. Outliers are less likely to significantly affect the ensemble's predictions.\n",
    "        \n",
    "4.Enhanced Generalization:\n",
    "\n",
    "    ~Ensemble methods generalize well to new, unseen data. They capture a broader range of patterns and relationships in \n",
    "     the data, making them more likely to make accurate predictions on data they haven't seen before.\n",
    "        \n",
    "5.Versatility:\n",
    "\n",
    "    ~Ensemble techniques can be applied to a wide range of machine learning algorithms and models, including decision trees,\n",
    "     neural networks, support vector machines, and more. This versatility allows practitioners to leverage the strengths \n",
    "    of different algorithms.\n",
    "    \n",
    "6.Stability and Reliability:\n",
    "\n",
    "    ~Ensembles tend to produce more stable and reliable predictions compared to individual models. They are less sensitive\n",
    "     to small changes in the training data, which is essential in real-world applications.\n",
    "        \n",
    "7.Interpretability:\n",
    "\n",
    "    ~Some ensemble methods, such as Random Forests, provide insights into feature importance, making them useful for \n",
    "     feature selection and understanding which features contribute most to predictions.\n",
    "        \n",
    "8.Winning Machine Learning Competitions:\n",
    "\n",
    "    ~Ensembles are a common and effective strategy in machine learning competitions (Kaggle, etc.) because they frequently\n",
    "     lead to improved performance and can provide an edge in highly competitive settings.\n",
    "        \n",
    "9.Handling Imbalanced Data:\n",
    "\n",
    "    ~Ensembles can help address imbalanced datasets by giving more weight to the minority class in classification problems,\n",
    "     making it easier to classify rare events accurately.\n",
    "        \n",
    "10.Model Selection and Tuning:\n",
    "\n",
    "    ~Ensembles can assist with model selection by trying different models as base learners and combining them. They can \n",
    "     also provide insights into hyperparameter tuning.\n",
    "        \n",
    "11.Reduction of Bias:\n",
    "\n",
    "    ~In boosting ensembles, models are trained to correct the errors of previous models, which can help reduce bias and \n",
    "     improve the accuracy of predictions.\n",
    "        \n",
    "12.Parallelization:\n",
    "\n",
    "    ~Many ensemble techniques allow for parallelization, speeding up training and prediction processes, especially in\n",
    "     distributed computing environments.\n",
    "        \n",
    "Overall, ensemble techniques are valuable tools in machine learning because they harness the collective wisdom of multiple\n",
    "models to enhance predictive accuracy, robustness, and generalization. They are particularly useful when the base models\n",
    "have complementary strengths and weaknesses, allowing the ensemble to compensate for individual model limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55dab2-a4b3-4c63-a37a-fb7bb47795ca",
   "metadata": {},
   "source": [
    "## Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383439d2-3694-47c3-adc4-b1b98ed42766",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble techniques are powerful tools in machine learning, but whether they are always better than individual models \n",
    "depends on various factors and the specific context of the problem at hand. Here are some considerations:\n",
    "\n",
    "1.Quality of Base Models:\n",
    "\n",
    "    ~The effectiveness of an ensemble often depends on the quality of the base models. If the base models are weak or poorly\n",
    "     performing, combining them in an ensemble may not necessarily yield better results. In such cases, improving the base \n",
    "    models themselves may be a better strategy.\n",
    "    \n",
    "2.Nature of the Data:\n",
    "\n",
    "    ~The nature of the data can influence the performance of ensembles. For example, if the dataset is small and relatively\n",
    "     simple, a single well-tuned model might suffice, and using an ensemble could introduce unnecessary complexity.\n",
    "        \n",
    "3.Overfitting:\n",
    "\n",
    "    ~Ensembles are less prone to overfitting than individual models, but overfitting can still occur if the ensemble is\n",
    "     too complex or if not enough regularization is applied. It's essential to monitor and control overfitting when using \n",
    "    ensembles.\n",
    "    \n",
    "4.Computation and Resource Constraints:\n",
    "\n",
    "    ~Building and maintaining ensembles can be computationally expensive, especially when dealing with a large number of \n",
    "     base models. In situations where computational resources are limited, using a single model may be more practical.\n",
    "        \n",
    "5.Interpretability:\n",
    "\n",
    "    ~Ensembles can be less interpretable than individual models, especially when combining a diverse set of base models. \n",
    "     In cases where interpretability is crucial, a single, simpler model may be preferred.\n",
    "        \n",
    "6.Diminishing Returns:\n",
    "\n",
    "    ~There can be a point of diminishing returns when adding more base models to an ensemble. Beyond a certain point, the\n",
    "     additional complexity may not significantly improve performance and might even degrade it due to overfitting.\n",
    "        \n",
    "7.Domain Knowledge:\n",
    "\n",
    "    ~Domain expertise can play a significant role in model selection. In some cases, domain-specific knowledge may suggest \n",
    "     that a particular algorithm or approach is better suited to the problem than an ensemble.\n",
    "        \n",
    "8.Data Quality and Preprocessing:\n",
    "\n",
    "    ~Data quality and preprocessing can impact the performance of models and ensembles. If data quality issues are not\n",
    "     adequately addressed, ensembles may not provide substantial improvements.\n",
    "        \n",
    "9.Ensemble Method Choice:\n",
    "\n",
    "    ~The choice of ensemble method matters. Some ensemble methods may work better for specific types of data or problems,\n",
    "     and choosing the wrong ensemble method may lead to suboptimal results.\n",
    "        \n",
    "In practice, it's essential to consider the trade-offs and conduct experiments to determine whether an ensemble approach is\n",
    "beneficial for a particular problem. Ensemble techniques are valuable and often lead to improved performance, especially \n",
    "when individual models have complementary strengths and weaknesses. However, they are not a one-size-fits-all solution, and\n",
    "careful consideration should be given to the specific problem, the data, and the resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a32f1-6b32-425f-8422-fca4a0a11861",
   "metadata": {},
   "source": [
    "## Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc79014-26bb-48ba-b5d7-b9cf8c9d4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95% CI) for Mean: [15.9975 21.7025]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your original data\n",
    "data = np.array([12, 14, 15, 16, 18, 20, 21, 22, 25, 27])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Initialize an array to store sample means\n",
    "sample_means = np.zeros(num_samples)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(num_samples):\n",
    "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "    sample_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the confidence interval (e.g., 95% CI)\n",
    "confidence_interval = np.percentile(sample_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval (95% CI) for Mean:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb7699-1119-490d-895f-518699493548",
   "metadata": {},
   "source": [
    "## Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28009e86-7078-471e-8087-29f5f3718ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bootstrap is a powerful statistical resampling technique used to estimate the sampling distribution of a statistic and to\n",
    "perform statistical inference when the underlying population distribution is unknown or when you have a limited sample size.\n",
    "It works by creating multiple resampled datasets (bootstrap samples) from the original data and using these samples to \n",
    "estimate various statistical properties or make inferences. Here are the steps involved in the bootstrap method:\n",
    "\n",
    "1.Original Data:\n",
    "\n",
    "    ~Start with your original dataset, which contains a sample of data points. This sample is assumed to be representative\n",
    "     of some population.\n",
    "        \n",
    "2.Resampling:\n",
    "\n",
    "    ~Perform random sampling with replacement from the original dataset to create multiple bootstrap samples. Each bootstrap\n",
    "     sample has the same size as the original dataset but may contain duplicate data points.\n",
    "    ~The key idea is to simulate the process of drawing samples from the population as if you had collected multiple\n",
    "     independent samples.\n",
    "        \n",
    "3.Statistic Calculation:\n",
    "\n",
    "    ~For each bootstrap sample, calculate the statistic or parameter of interest. This statistic can be any measure that\n",
    "     you want to estimate or test (e.g., mean, median, standard deviation, correlation coefficient, etc.).\n",
    "    ~This step results in a distribution of statistics obtained from the different bootstrap samples.\n",
    "    \n",
    "4.Sampling Distribution Estimation:\n",
    "\n",
    "    ~Analyze the distribution of the statistics calculated from the bootstrap samples.\n",
    "    ~Typically, this distribution is used to estimate the properties of the statistic you are interested in, such as its\n",
    "     mean, standard error, and confidence interval.\n",
    "        \n",
    "5.Confidence Interval Calculation (Optional):\n",
    "\n",
    "    ~If you want to estimate a confidence interval for the statistic, you can use percentiles from the bootstrap\n",
    "     distribution.\n",
    "    ~For example, to calculate a 95% confidence interval, you would find the 2.5th and 97.5th percentiles of the bootstrap\n",
    "     statistics.\n",
    "        \n",
    "6.Hypothesis Testing (Optional):\n",
    "\n",
    "    ~Bootstrap can be used for hypothesis testing. To perform hypothesis tests, you compare the observed statistic (from\n",
    "     your original data) to the distribution of the statistic obtained from the bootstrap samples.\n",
    "    ~The p-value is often used to determine the likelihood of observing the statistic or a more extreme value under the \n",
    "     null hypothesis.\n",
    "        \n",
    "7.Interpretation:\n",
    "\n",
    "    ~Interpret the results in the context of your analysis. For example, you might conclude that there is evidence to\n",
    "     suggest a parameter differs from a certain value or that a statistic falls within a specific range with a certain\n",
    "    level of confidence.\n",
    "    \n",
    "Bootstrap is a valuable tool in statistics and data analysis because it allows you to obtain estimates and perform inference\n",
    "without making strong parametric assumptions about the underlying data distribution. It provides a non-parametric and data-\n",
    "driven approach to statistical analysis, making it widely applicable in various fields, including hypothesis testing,\n",
    "parameter estimation, and model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853b7d1-97c5-4d9b-884b-e0378d83e02e",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cfeb73-577f-42a2-820f-9da55e19993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Mean Height: [15. 15.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data (sample of tree heights)\n",
    "original_data = np.array([15] * 50)  # Use the sample mean as the original data\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(num_bootstrap_samples):\n",
    "    # Generate a bootstrap sample by resampling with replacement\n",
    "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means[i] = bootstrap_mean\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
